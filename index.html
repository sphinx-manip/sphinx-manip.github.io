<!DOCTYPE html>
<html>
  <head>
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-VQTBKP87MK"
    ></script>

    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-VQTBKP87MK");
    </script>

    <meta charset="utf-8" />
    <meta
      name="description"
      content="SPHINX"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      What's the Move?
      Hybrid Imitation Learning via Salient Points
    </title>



    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">
      google.load("jquery", "1.3.2");
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [['$', '$']]
          },
          "HTML-CSS": {
          availableFonts: ["TeX", "STIX-Web", "Asana-Math", "Latin-Modern"], // Specify the desired font here
          preferredFont: "STIX-Web", // Set the preferred font
          webFont: "STIX-Web" // Set the web font to use
          },
      });
    </script>
    <script
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
      type="text/javascript"
    ></script>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                What's the Move?<br />Hybrid Imitation 
                Learning via Salient Points
              </h1>


              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a target="_blank" href="http://priya.sundaresan.us">Priya Sundaresan*,</a><sup>1</sup>
                  <a target="_blank" href="https://hengyuanhu.com/">Hengyuan Hu*,</a><sup>1</sup>
                  <a target="_blank" href="https://quanvuong.github.io/">Quan Vuong,</a><sup>2</sup>
                  <a target="_blank" href="https://web.stanford.edu/~bohg/">Jeannette Bohg,</a><sup>1</sup>
                  <a target="_blank" href="https://dorsa.fyi/">Dorsa Sadigh</a><sup>1</sup>
                </span>
              </div>
    
              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup><font size="-0.4">*</sup>Equal contribution</font></span><br>
                <span class="author-block"><sup>1</sup>Stanford University,</span>
                <span class="author-block"><sup>2</sup>Physical Intelligence</span>
              </div>
    
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a
                      target="_blank"
                      href="assets/sphinx.pdf"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>PDF</span>
                    </a>
                  </span>
    
                  <!-- Code Link. -->
                  <span class="link-block">
                    <a
                      target="_blank"
                      href="https://github.com/priyasundaresan/sphinx"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fa-brands fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                </div>
              </div>
    
            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- Title Card + Caption -->
    <section class="hero teaser">
      <div class="container is-fullhd">
        <div class="hero-body">
          <div class="container">
            <div class="columns is-vcentered  is-centered">
              <video
                id="teaser"
                class="intro-video"
                autoplay
                loop
                muted
                height="100%"
              >
                <source src="media/builds/teaser.mp4" type="video/mp4" />
              </video>
            </div>
            <br />
            <div class="columns is-vcentered  is-centered shaded-panel" style="width: 70%;">
            <h2 class="subtitle has-text-centered">
              <strong><span class="sphinxtitle">Sphinx</span> (<span style="text-decoration: underline;">S</span>alient <span style="text-decoration: underline;">P</span>oints for <span style="text-decoration: underline;">H</span>ybrid <span style="text-decoration: underline;">I</span>mitatio<span style="text-decoration: underline;">N</span> and e<span style="text-decoration: underline;">X</span>ecution)
              <br>is a hybrid IL agent that learns to switch between two modes of execution:</strong>
              <br><br>(<span style="display: inline-block; width: 13px; height: 13px; background-color: #d96704;"></span>) a <b>point cloud-based waypoint policy</b> for long-range movement &
              <br>(<span style="display: inline-block; width: 13px; height: 13px; background-color: #ffd28a;"></span>) a <b>wrist-image based dense policy</b>  for more precise manipulation.
              <br><br>Learned <font color='red'>salient points</font> guide the selection of input modality / action space, <br>enabling
              <b>complex manipulation</b> with <b>visuospatial generalization</b>.
            </h2>
            </div>
          </div>
        </div>
      </div>
    </section>
<hr>

    <h2 class="subtitle has-text-centered">
      <br />
      <span class="sphinxtitle">Sphinx</span> tackles long-horizon, precise tasks while generalizing to <strong>unseen spatial arrangements, visual distractors, <br>novel camera viewpoints, and execution speeds,</strong> even having only been trained on 20-60 demos.
    </h2>

      <div class="columns shaded-panel">
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/train.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Train Track</h3>
        </div>
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/coffee.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Coffee Making</h3>
        </div>
        <div class="column has-text-centered">
          <video id="cups2"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/cups_distractors.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Cup Stack (Visual Distractors)</h3>
        </div>
        </div>

      <div class="columns shaded-panel">
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/cups_viewpoint.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Cup Stack (Novel Viewpoint)</h3>
        </div>
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/cups_speed.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Cup Stack (Novel Speed)</h3>
        </div>
        <div class="column has-text-centered">
          <video id="cups2"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/drawer.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Drawer (Novel Height)</h3>
        </div>
        </div>
    </div>



    <hr />

    <!-- Abstract. -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
While imitation learning (IL) offers a promising framework for teaching robots various behaviors, learning complex tasks remains challenging. Existing IL policies struggle to generalize effectively across visual and spatial variations even for simple tasks. In this work, we introduce <span class="sphinxtitle">Sphinx</span>: Salient Point-based Hybrid ImitatioN and eXecution, a flexible IL policy that leverages multimodal observations (point clouds and wrist images), along with a hybrid action space of low-frequency, sparse waypoints and high-frequency, dense end effector movements. Given 3D point cloud observations, <span class="sphinxtitle">Sphinx</span> learns to infer task-relevant points within a point cloud, or salient points, which support spatial generalization by focusing on semantically meaningful features. These salient points serve as anchor points to predict waypoints for long-range movement, such as reaching target poses in free-space. Once near a salient point, <span class="sphinxtitle">Sphinx</span> learns to switch to predicting dense end-effector movements given close-up wrist images for precise phases of a task. By exploiting the strengths of different input modalities and action representations for different manipulation phases, <span class="sphinxtitle">Sphinx</span> tackles complex tasks in a sample-efficient, generalizable manner.
              <br><br>Our method achieves <strong>86.7%</strong> success across <strong>4 real-world</strong> and <strong>2 simulated</strong> tasks, outperforming the next best state-of-the-art IL baseline by <strong>41.1%</strong> on average across <strong>440 real world trials</strong>. <span class="sphinxtitle">Sphinx</span> additionally generalizes to novel viewpoints, visual distractors, spatial arrangements, and execution speeds with a <strong>1.7x</strong> speedup over the most competitive baseline. 
              </p>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/TB0g52N-3_Y?rel=0&amp;showinfo=0"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div> -->
    </section>

    <hr />
    <!-- System Overview -->
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Video -->
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-3">SPHINX Method Overview</h2>


            <div class="shaded-panel" style="background: white;">
            <div class="columns is-vcentered is-centered">
            <img src="media/images/overview.png" style="width: 100%;"/>
            </div>
            <div class="columns is-centered has-text-justified">
            <p>
            <br/>
            <span class="sphinxtitle">Sphinx</span> learns to switch between two modes (\(m_t\)) of execution: in <font color='#d45800'>waypoint mode</font>, the waypoint policy predicts a waypoint (\(w_t\)) as an offset (\(\phi_t\)) to a salient point \(z_t\) (e.g., mug handle, coffee pod) using a point cloud. Once the waypoint is reached, <span class="sphinxtitle">Sphinx</span> switches to a wrist-camera image-based Diffusion Policy which predicts <font color='orange'>dense</font> actions (\(a_t\)) for precise manipulation around a salient point. On the right, <span class="sphinxtitle">Sphinx</span> interleaves both modes of execution to complete a long-horizon coffee-making task guided by salient points (<span class="spcircle"></span>) and mode switches (<span style="display: inline-block; width: 13px; height: 13px; background-color: #d1d1d1; border: 2px solid black"></span>)<br>.
            <p>
            </div>

            <h4 class="title is-5"><font color='#e37600'>Waypoint Policy Architecture</font></h4>
            <div class="columns is-vcentered is-centered">
            <img src="media/images/waypoint_model.png" style="width: 100%;"/>
            </div>

            <div class="columns is-centered has-text-justified">
            <p>
            <br/>
            <span class="sphinxtitle">Sphinx</span>'s waypoint policy is a GPT-2 style Transformer backbone which takes downsampled point clouds as inputs and predicts: per-point salient probabilities, per-point waypoint offsets, and the end-effector rotation, gripper, and next mode as separate tokens. At test-time, we take the predicted point with highest saliency probability as the salient point, and use the offset corresponding to this point to recover a full waypoint action. Thus, during training, we only penalize offset prediction on high-probability salient points.
            <p>
            <br />
            </div>
            </div>
            </div>

          </div>
        </div>
      </div>
    </section>
    <hr />

    <!-- Data Collection -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h2 class="title is-3">SPHINX Data Collection Interface</h2>

            <div class="shaded-panel" style="background: white;">
            <h4 class="title is-5"><font color='#e37600'>Ex: Teleoperating the Train Track Task</font></h4>
            <video
              id="system-overview-video"
              class="data-collection-video"
              autoplay
              muted
              loop
            >
              <source src="media/interface/interface.mp4" type="video/mp4" />
            </video>
            <div class="content has-text-justified">
              <p>
              </p>
            </div>

            <div class="columns is-centered has-text-justified">
            <p>
            We design a data collection interface which allows a demonstrator to specify salient points and waypoints with simple click-and-drag interactions in a web UI, reachable with a controller, and dense actions using a SpaceMouse:
            <br/>
            <br/>
            <p>
            </div>

            <div class="columns is-vcentered is-centered">
            <img src="media/images/interface.png" style="width: 100%;"/>
            </div>


            </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <hr />

    <!-- Results -->
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Video -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Evaluations</h2>
            <div class="columns is-vcentered is-centered"></div>
          </div>
        </div>

        <!-- Description and List -->
        <div class="columns is-centered shaded-panel" style="width: 90%;">
          <div class="column">
            <p>
              We compare <span class="sphinxtitle">Sphinx</span> on 4 real and 2 simulated tasks
              against several SOTA IL algorithms:<br><br>
            </p>
            <ul style="text-align: left;">
              <li>
                <strong>Dense-Only Baselines:</strong> Diffusion Policy (DP) and 3D Diffusion Policy are state-of-the-art IL policies which only differ in their input modality (third person + wrist images vs. pointclouds, respectively). OpenVLA is a third-person image-based dense-only policy which is pre-trained on the large-scale Open X-Embodiment data and finetuned on our datasets.
              </li>
              <li>
                <strong>Hybrid Baseline</strong>: HYDRA learns to switch amongst waypoint and dense modes, but critically only takes images (third-person + wrist) as input and does not predict salient points/offsets.
              </li>
              <li>
                <strong>Waypoint-Only Baselines</strong>: We compare SPHINX against two variants of its <em>waypoint</em> policy which omit the dense mode entirely. Vanilla Waypoint predict waypoints directly given point cloud input, disregarding salient points entirely, and Vanilla Waypoint Aux. SP. predicts waypoints directly but adds salient prediction as an auxiliary task during training alone.
              </li>
            </ul>
          </div>
        </div>
        <br>

        <!-- Table -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <table
              id="methods_comparison"
              style="width: 100%;  border-collapse: collapse; margin-top: -10px; margin-bottom: -10px;"
            >
              <thead>
                <tr>
                  <th
                    class="centered-header"
                    style="border-bottom: 2px solid black;"
                  >
                    Method
                  </th>
                  <th
                    class="centered-header"
                    style="border-bottom: 2px solid black;"
                  >
                    Waypoint<br />
                    Mode
                  </th>
                  <th
                    class="centered-header"
                    style="border-bottom: 2px solid black;"
                  >
                    Dense<br />
                    Mode
                  </th>
                  <th
                    class="centered-header"
                    style="border-bottom: 2px solid black;"
                  >
                    Point Cloud <br />
                    Input
                  </th>
                  <th
                    class="centered-header"
                    style="border-bottom: 2px solid black;"
                  >
                    Image <br />
                    Input
                  </th>
                  <th
                    class="centered-header"
                    style="border-bottom: 2px solid black;"
                  >
                    Salient<br />
                    Points
                  </th>
                  <th
                    class="centered-header"
                    style="border-bottom: 2px solid black;"
                  >
                    Offset Action<br />
                    Parameterization
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="border-right: 2px solid black;">
                    <a
                      href="https://diffusion-policy.cs.columbia.edu/"
                      target="_blank"
                      >DP</a
                    >
                  </td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10007;</td>
                </tr>
                <tr>
                  <td style="border-right: 2px solid black;">
                    <a
                      href="https://3d-diffusion-policy.github.io/"
                      target="_blank"
                      >3D DP</a
                    >
                  </td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10007;</td>
                  <td>&#10007;</td>
                </tr>
                <tr>
                  <td style="border-right: 2px solid black;">
                    <a
                      href="https://openvla.github.io/"
                      target="_blank"
                      >Fine-tuned OpenVLA</a
                    >
                  </td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10007;</td>
                </tr>
                <tr>
                  <td style="border-right: 2px solid black;">
                    <a
                      href="https://sites.google.com/view/hydra-il-2023"
                      target="_blank"
                      >Hydra</a
                    >
                  </td>
                  <td>&#10003;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10007;</td>
                </tr>

                <tr>
                  <td style="border-right: 2px solid black;">
                  Vanilla Waypoint
                  </td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10007;</td>
                  <td>&#10007;</td>
                </tr>

                <tr>
                  <td style="border-right: 2px solid black;">
                  Vanilla Waypoint + Aux. SP
                  </td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                  <td>&#10003;</td>
                  <td>&#10007;</td>
                </tr>

                <tr style="">
                  <td
                    style="border-right: 2px solid black; background-color: #FFA5004D;"
                  >
                    <strong>SPHINX</strong>
                  </td>
                  <td style="background-color: #FFA5004D;">&#10003;</td>
                  <td style="background-color: #FFA5004D;">&#10003;</td>
                  <td style="background-color: #FFA5004D;">&#10003;</td>
                  <td style="background-color: #FFA5004D;">&#10003;</td>
                  <td style="background-color: #FFA5004D;">&#10003;</td>
                  <td style="background-color: #FFA5004D;">&#10003;</td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>

        <hr />

        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h3 class="title is-4">Hybrid Task Results</h3>
          </div>
        </div>

        <div class="chart-container" style="width: 70%; margin: auto;">
            <div class="columns is-vcentered  is-centered shaded-panel">
            <h2 class="subtitle has-text-centered">
              <strong><font color='#242424'>Click on any bar to see the videos!</font></strong>
            </h2>
            </div>


          <br />
          <canvas id="taskBarChart"></canvas>
          <div id="video-container" style="margin-top: 20px;"></div>
        </div>

        <br/>
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <h3 class="title is-4">Waypoint Task Results</h3>
          </div>
        </div>

        <div class="chart-container" style="width: 70%; margin: auto;">
          <canvas id="taskBarChartWaypoint"></canvas>
        </div>


        <div class="columns is-centered">
          <div class="content">
            <p>
            </p>
          </div>
        </div>


            <br/>
            <div class="columns is-centered has-text-justified shaded-panel">
            <p>
            <span class="sphinxtitle">Sphinx</span> outperforms baselines across 4 real-world and 3 simulated tasks by 41.1% on average. By effectively leveraging mode switches, <span class="sphinxtitle">Sphinx</span> can effectively execute long-range movements with high precision, while also better navigating difficult bottleneck states such as coffee pod insertion or placing the train on the bridge. Dense-only baselines struggle with long-horizon execution and spatial reasoning, while waypoint-only baselines cannot perform complex manipulation. <br><br>Other policies that omit salient point-offset prediction (Vanilla Waypoint) or do not exploit both point clouds and wrist images (HYDRA) exhibit more imprecision and are thus more error-prone.
            <br/>
            <p>
            </div>
    <hr />
        <div class="columns is-centered has-text-centered">

          <div class="column is-four-fifths">
            <h3 class="title is-4">SPHINX Generalization Capabilities</h3>
          </div>

        </div>

        <div class="columns is-centered has-text-centered">

          <div class="column is-four-fifths">
            <h4 class="title is-5"><font color='#e37600'>Spatial Generalization</font></h4>
          </div>

        </div>


      <div class="shaded-panel">
      <div class="columns">
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/generalization/spatial/drawer.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Drawer</h3>
        </div>
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/generalization/spatial/cups.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Cup Stack</h3>
        </div>
      </div>
      <div class="columns">
        <div class="column has-text-centered">
          <video id="cups2"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/generalization/spatial/coffee.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Coffee Making</h3>
        </div>
        <div class="column has-text-centered">
          <video id="cups2"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/generalization/spatial/train.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Train Track</h3>
        </div>
        </div>
            <div class="columns is-centered has-text-justified">
            <p>
            <br/>
            We visualize the distribution of initial states across successful <span class="sphinxtitle">Sphinx</span> rollouts, spanning a wide range of object positions and orientations across the table.
            <br/>
            <p>
            </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column">
            <br><h4 class="title is-5"><font color='#e37600'>Visual Generalization</font></h4>
          </div>
        </div>


      <div class="shaded-panel">
      <div class="columns">
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/cups_distractors.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Visual Distractors</h3>
        </div>
        <div class="column has-text-centered">
          <video id="cups"
            controls
            muted
            autoplay
            loop
            width="100%">
            <source src="media/rollouts/cups_viewpoint.mp4" 
            type="video/mp4">
          </video>
          <h3 class="title is-5">Novel Viewpoint</h3>
        </div>
      </div>
            <div class="columns is-centered has-text-justified">
            <p>
            <br/>
            Using (calibrated) point clouds enables <span class="sphinxtitle">Sphinx</span>'s waypoint policy to be robust to new camera viewpoints. The wrist-based dense policy is unaffected by this change, as well as visual distractors in the surrounding scene.
            <br/>
            <p>
            </div>
      </div>

        <div class="columns is-centered has-text-centered">
          <div class="column">
            <br><h4 class="title is-5"><font color='#e37600'>Execution Speeds</font></h4>
          </div>


        </div>

      <div class="shaded-panel">
      <div class="columns">
        <div class="column has-text-centered">
          <p>⏳⌛ <strong>Time Elapsed:</strong> <span id="timeCounter">0:00</span><br><br></p>
          <video id="speedcomparison"
            controls
            muted
            autoplay
            loop
            width="80%">
            <source src="media/generalization/speed/speed.mp4" 
            type="video/mp4">
          </video>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const video = document.getElementById('speedcomparison');
            const timeCounter = document.getElementById('timeCounter');

            // Function to format the time as minutes and seconds
            function formatTime(seconds) {
                const minutes = Math.floor(seconds / 60);
                const remainingSeconds = Math.floor(seconds % 60);
                return `${minutes}:${remainingSeconds < 10 ? '0' : ''}${remainingSeconds}`;
            }

            // Update the counter each time the video's time changes
            video.addEventListener('timeupdate', () => {
                const currentTime = video.currentTime;
                timeCounter.textContent = formatTime(currentTime);
            });
        });
    </script>

          
        </div>

        </div>
            <div class="columns is-centered has-text-justified">
            <p>
            <br/>
            Since <span class="sphinxtitle">Sphinx</span> uses a controller to execute waypoint actions, we can arbitrarily change the max positional delta of the controller at test time to enable faster execution speeds than the rate data was collected at. By doubling the max delta of the controller at test-time, <span class="sphinxtitle">Sphinx</span> can achieve a <strong>1.7x</strong> speedup over Diffusion Policy, while achieving far higher success.
            <br/>
            <p>
            </div>
      </div>



  </body>
</html>
